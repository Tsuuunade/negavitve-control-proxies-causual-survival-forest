{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f9f73e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Add parent directory to Python path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "parent_dir = Path.cwd().parent\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "from nc_csf.models import NCCausalForestDML, NCCausalForestDMLOracle, BaselineCausalForestDML\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "be56a76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (5735, 63)\n",
      "Columns: ['Unnamed: 0', 'cat1', 'cat2', 'ca', 'sadmdte', 'dschdte', 'dthdte', 'lstctdte', 'death', 'cardiohx', 'chfhx', 'dementhx', 'psychhx', 'chrpulhx', 'renalhx', 'liverhx', 'gibledhx', 'malighx', 'immunhx', 'transhx', 'amihx', 'age', 'sex', 'edu', 'surv2md1', 'das2d3pc', 't3d30', 'dth30', 'aps1', 'scoma1', 'meanbp1', 'wblc1', 'hrt1', 'resp1', 'temp1', 'pafi1', 'alb1', 'hema1', 'bili1', 'crea1', 'sod1', 'pot1', 'paco21', 'ph1', 'swang1', 'wtkilo1', 'dnr1', 'ninsclas', 'resp', 'card', 'neuro', 'gastr', 'renal', 'meta', 'hema', 'seps', 'trauma', 'ortho', 'adld3p', 'urin1', 'race', 'income', 'ptid']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>ca</th>\n",
       "      <th>sadmdte</th>\n",
       "      <th>dschdte</th>\n",
       "      <th>dthdte</th>\n",
       "      <th>lstctdte</th>\n",
       "      <th>death</th>\n",
       "      <th>cardiohx</th>\n",
       "      <th>...</th>\n",
       "      <th>meta</th>\n",
       "      <th>hema</th>\n",
       "      <th>seps</th>\n",
       "      <th>trauma</th>\n",
       "      <th>ortho</th>\n",
       "      <th>adld3p</th>\n",
       "      <th>urin1</th>\n",
       "      <th>race</th>\n",
       "      <th>income</th>\n",
       "      <th>ptid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>COPD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>11142</td>\n",
       "      <td>11151.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11382</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>white</td>\n",
       "      <td>Under $11k</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MOSF w/Sepsis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>11799</td>\n",
       "      <td>11844.0</td>\n",
       "      <td>11844.0</td>\n",
       "      <td>11844</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1437.0</td>\n",
       "      <td>white</td>\n",
       "      <td>Under $11k</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MOSF w/Malignancy</td>\n",
       "      <td>MOSF w/Sepsis</td>\n",
       "      <td>Yes</td>\n",
       "      <td>12083</td>\n",
       "      <td>12143.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12400</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>599.0</td>\n",
       "      <td>white</td>\n",
       "      <td>$25-$50k</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ARF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>11146</td>\n",
       "      <td>11183.0</td>\n",
       "      <td>11183.0</td>\n",
       "      <td>11182</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>white</td>\n",
       "      <td>$11-$25k</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MOSF w/Sepsis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>12035</td>\n",
       "      <td>12037.0</td>\n",
       "      <td>12037.0</td>\n",
       "      <td>12036</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>white</td>\n",
       "      <td>Under $11k</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               cat1           cat2   ca  sadmdte  dschdte  \\\n",
       "0           1               COPD            NaN  Yes    11142  11151.0   \n",
       "1           2      MOSF w/Sepsis            NaN   No    11799  11844.0   \n",
       "2           3  MOSF w/Malignancy  MOSF w/Sepsis  Yes    12083  12143.0   \n",
       "3           4                ARF            NaN   No    11146  11183.0   \n",
       "4           5      MOSF w/Sepsis            NaN   No    12035  12037.0   \n",
       "\n",
       "    dthdte  lstctdte death  cardiohx  ...  meta  hema  seps  trauma  ortho  \\\n",
       "0      NaN     11382    No         0  ...    No    No    No      No     No   \n",
       "1  11844.0     11844   Yes         1  ...    No    No   Yes      No     No   \n",
       "2      NaN     12400    No         0  ...    No    No    No      No     No   \n",
       "3  11183.0     11182   Yes         0  ...    No    No    No      No     No   \n",
       "4  12037.0     12036   Yes         0  ...    No    No    No      No     No   \n",
       "\n",
       "   adld3p   urin1   race      income  ptid  \n",
       "0     0.0     NaN  white  Under $11k     5  \n",
       "1     NaN  1437.0  white  Under $11k     7  \n",
       "2     NaN   599.0  white    $25-$50k     9  \n",
       "3     NaN     NaN  white    $11-$25k    10  \n",
       "4     NaN    64.0  white  Under $11k    11  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('rhc.csv')\n",
    "\n",
    "print(f\"Shape of the dataset: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aad49b",
   "metadata": {},
   "source": [
    "I found two papers online that uses RHC dataset. Their ways of choosing covariates X differs a bit, so I created two dataframes to run the test. Note that I didn't run the oracle model since we don't actually know the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf52e4bc",
   "metadata": {},
   "source": [
    "### Tchetgen Tchetgen, E. J., Ying, A., Cui, Y., Shi, X., and Miao, W. An introduction to proximal causal learning. arXiv preprint arXiv:2009.10982, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a87c523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final X shape: (5735, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>A</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>race_black</th>\n",
       "      <th>pafi1</th>\n",
       "      <th>paco21</th>\n",
       "      <th>ph1</th>\n",
       "      <th>hema1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>70.25098</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68.00000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>7.359375</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>78.17896</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>218.31250</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7.329102</td>\n",
       "      <td>32.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>46.09198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>275.50000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.359375</td>\n",
       "      <td>21.097656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>75.33197</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>156.65625</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7.459961</td>\n",
       "      <td>26.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>67.90997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>478.00000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.229492</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Y  A       age  sex  race_black      pafi1  paco21       ph1      hema1\n",
       "0  30  0  70.25098    0           0   68.00000    40.0  7.359375  58.000000\n",
       "1  30  1  78.17896    1           0  218.31250    34.0  7.329102  32.500000\n",
       "2  30  1  46.09198    1           0  275.50000    16.0  7.359375  21.097656\n",
       "3  30  0  75.33197    1           0  156.65625    30.0  7.459961  26.296875\n",
       "4   2  1  67.90997    0           0  478.00000    17.0  7.229492  24.000000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treatment A\n",
    "A_raw = df[\"swang1\"]\n",
    "if A_raw.dtype == \"O\":\n",
    "    A = (A_raw == \"RHC\").astype(int)\n",
    "else:\n",
    "    A = (A_raw.astype(float) > 0).astype(int)\n",
    "\n",
    "# Outcome Y\n",
    "Y = df[\"t3d30\"]\n",
    "\n",
    "# Covariates X\n",
    "X = pd.DataFrame({\n",
    "    \"age\": df[\"age\"],\n",
    "    \"sex\": df[\"sex\"],\n",
    "    \"race\": df[\"race\"]\n",
    "})\n",
    "\n",
    "if X[\"sex\"].dtype == \"O\":\n",
    "    X[\"sex\"] = (X[\"sex\"] == \"Female\").astype(int)\n",
    "\n",
    "if X[\"race\"].dtype == \"O\":\n",
    "    X[\"race_black\"] = (X[\"race\"] == \"black\").astype(int)\n",
    "    X = X.drop(columns=[\"race\"])\n",
    "\n",
    "# Proxies Z & W\n",
    "Z = df[[\"pafi1\", \"paco21\"]].copy() \n",
    "W = df[[\"ph1\", \"hema1\"]].copy()  \n",
    "\n",
    "analysis_cols = pd.concat(\n",
    "    [\n",
    "        Y.rename(\"Y\"),\n",
    "        A.rename(\"A\"),\n",
    "        X,\n",
    "        Z.rename(columns={\"pafi1\": \"pafi1\", \"paco21\": \"paco21\"}),\n",
    "        W.rename(columns={\"ph1\": \"ph1\", \"hema1\": \"hema1\"}),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "analysis_df = analysis_cols.dropna().copy()\n",
    "\n",
    "# Overwrite with cleaned arrays\n",
    "Y = analysis_df[\"Y\"].values\n",
    "A = analysis_df[\"A\"].values.astype(int)\n",
    "X_colnames = [col for col in analysis_df.columns if col not in [\"Y\", \"A\", \"pafi1\", \"paco21\", \"ph1\", \"hema1\"]]\n",
    "X = analysis_df[X_colnames]\n",
    "Z = analysis_df[[\"pafi1\", \"paco21\"]]\n",
    "W = analysis_df[[\"ph1\", \"hema1\"]]\n",
    "\n",
    "print(f\"\\nFinal X shape: {X.shape}\")\n",
    "analysis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e3a7a37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4014\n",
      "Test samples: 1721\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, A_train, A_test, Y_train, Y_test, Z_train, Z_test, W_train, W_test = train_test_split(\n",
    "    X.values, A, Y, Z.values, W.values, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9e5df696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline predictions - Mean: -1.8889, Std: 1.8212\n",
      "Baseline predictions - Min: -7.6396, Max: 1.5299\n"
     ]
    }
   ],
   "source": [
    "baseline = BaselineCausalForestDML(n_estimators=200, min_samples_leaf=20, random_state=42)\n",
    "baseline.fit_baseline(X_train, A_train, Y_train, verbose=False)\n",
    "pred_baseline = baseline.effect(X_test).ravel()\n",
    "\n",
    "print(f\"\\nBaseline predictions - Mean: {pred_baseline.mean():.4f}, Std: {pred_baseline.std():.4f}\")\n",
    "print(f\"Baseline predictions - Min: {pred_baseline.min():.4f}, Max: {pred_baseline.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "74f247ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NC-CSF predictions - Mean: -0.9961, Std: 1.8993\n",
      "NC-CSF predictions - Min: -6.7210, Max: 3.4750\n"
     ]
    }
   ],
   "source": [
    "nccsf = NCCausalForestDML(n_estimators=200, min_samples_leaf=20, cv=5, random_state=42)\n",
    "nccsf.fit(Y=Y_train, T=A_train, X=X_train, Z=Z_train, W=W_train)\n",
    "pred_nccsf = nccsf.effect(X_test).ravel()\n",
    "\n",
    "print(f\"\\nNC-CSF predictions - Mean: {pred_nccsf.mean():.4f}, Std: {pred_nccsf.std():.4f}\")\n",
    "print(f\"NC-CSF predictions - Min: {pred_nccsf.min():.4f}, Max: {pred_nccsf.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ebdb24",
   "metadata": {},
   "source": [
    "### Sverdrup, E., Cui, Y. Proximal Causal Learning of Conditional Average Treatment Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "327291ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final X shape: (5735, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>A</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cat1_coma</th>\n",
       "      <th>cat2_coma</th>\n",
       "      <th>dnr1</th>\n",
       "      <th>surv2md1</th>\n",
       "      <th>aps1</th>\n",
       "      <th>pafi1</th>\n",
       "      <th>paco21</th>\n",
       "      <th>ph1</th>\n",
       "      <th>hema1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>70.25098</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.640991</td>\n",
       "      <td>46</td>\n",
       "      <td>68.00000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>7.359375</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>78.17896</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>50</td>\n",
       "      <td>218.31250</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7.329102</td>\n",
       "      <td>32.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>46.09198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.317000</td>\n",
       "      <td>82</td>\n",
       "      <td>275.50000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.359375</td>\n",
       "      <td>21.097656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>75.33197</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.440979</td>\n",
       "      <td>48</td>\n",
       "      <td>156.65625</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7.459961</td>\n",
       "      <td>26.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>67.90997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.437000</td>\n",
       "      <td>72</td>\n",
       "      <td>478.00000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.229492</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Y  A       age  sex  cat1_coma  cat2_coma  dnr1  surv2md1  aps1  \\\n",
       "0  30  0  70.25098    0          0          0     0  0.640991    46   \n",
       "1  30  1  78.17896    1          0          0     0  0.755000    50   \n",
       "2  30  1  46.09198    1          0          0     0  0.317000    82   \n",
       "3  30  0  75.33197    1          0          0     0  0.440979    48   \n",
       "4   2  1  67.90997    0          0          0     1  0.437000    72   \n",
       "\n",
       "       pafi1  paco21       ph1      hema1  \n",
       "0   68.00000    40.0  7.359375  58.000000  \n",
       "1  218.31250    34.0  7.329102  32.500000  \n",
       "2  275.50000    16.0  7.359375  21.097656  \n",
       "3  156.65625    30.0  7.459961  26.296875  \n",
       "4  478.00000    17.0  7.229492  24.000000  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treatment A\n",
    "A_raw = df[\"swang1\"]\n",
    "if A_raw.dtype == \"O\":\n",
    "    A = (A_raw == \"RHC\").astype(int)\n",
    "else:\n",
    "    A = (A_raw.astype(float) > 0).astype(int)\n",
    "\n",
    "# Outcome Y\n",
    "Y = df[\"t3d30\"]\n",
    "\n",
    "# Covariates X\n",
    "# Note we define cat1_coma and cat2_coma by ourselves since it doesn't exist in the original dataset\n",
    "# reference: https://search.r-project.org/CRAN/refmans/ATbounds/html/RHC.html\n",
    "X = pd.DataFrame({\n",
    "    \"age\": df[\"age\"],\n",
    "    \"sex\": df[\"sex\"],\n",
    "    \"cat1_coma\": df[\"cat1\"].apply(lambda x: 1 if x in [\"Coma\"] else 0),\n",
    "    \"cat2_coma\": df[\"cat2\"].apply(lambda x: 1 if x in [\"Coma\"] else 0),\n",
    "    \"dnr1\": df[\"dnr1\"],\n",
    "    \"surv2md1\": df[\"surv2md1\"],\n",
    "    \"aps1\": df[\"aps1\"],\n",
    "})\n",
    "\n",
    "if X[\"sex\"].dtype == \"O\":\n",
    "    X[\"sex\"] = (X[\"sex\"] == \"Female\").astype(int)\n",
    "\n",
    "if X[\"dnr1\"].dtype == \"O\":\n",
    "    X[\"dnr1\"] = X[\"dnr1\"].map({\"Yes\": 1, \"No\": 0}).fillna(0).astype(int)\n",
    "\n",
    "# Proxies Z & W\n",
    "Z = df[[\"pafi1\", \"paco21\"]].copy() \n",
    "W = df[[\"ph1\", \"hema1\"]].copy()  \n",
    "\n",
    "analysis_cols = pd.concat(\n",
    "    [\n",
    "        Y.rename(\"Y\"),\n",
    "        A.rename(\"A\"),\n",
    "        X,\n",
    "        Z.rename(columns={\"pafi1\": \"pafi1\", \"paco21\": \"paco21\"}),\n",
    "        W.rename(columns={\"ph1\": \"ph1\", \"hema1\": \"hema1\"}),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "analysis_df = analysis_cols.dropna().copy()\n",
    "\n",
    "# Overwrite with cleaned arrays\n",
    "Y = analysis_df[\"Y\"].values\n",
    "A = analysis_df[\"A\"].values.astype(int)\n",
    "X_colnames = [col for col in analysis_df.columns if col not in [\"Y\", \"A\", \"pafi1\", \"paco21\", \"ph1\", \"hema1\"]]\n",
    "X = analysis_df[X_colnames]\n",
    "Z = analysis_df[[\"pafi1\", \"paco21\"]]\n",
    "W = analysis_df[[\"ph1\", \"hema1\"]]\n",
    "\n",
    "print(f\"\\nFinal X shape: {X.shape}\")\n",
    "analysis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "77d3a67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4014\n",
      "Test samples: 1721\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, A_train, A_test, Y_train, Y_test, Z_train, Z_test, W_train, W_test = train_test_split(\n",
    "    X.values, A, Y, Z.values, W.values, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe7164a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline predictions - Mean: -1.2659, Std: 1.3492\n",
      "Baseline predictions - Min: -5.8837, Max: 3.0109\n"
     ]
    }
   ],
   "source": [
    "baseline = BaselineCausalForestDML(n_estimators=200, min_samples_leaf=20, random_state=42)\n",
    "baseline.fit_baseline(X_train, A_train, Y_train, verbose=True)\n",
    "pred_baseline = baseline.effect(X_test).ravel()\n",
    "\n",
    "print(f\"\\nBaseline predictions - Mean: {pred_baseline.mean():.4f}, Std: {pred_baseline.std():.4f}\")\n",
    "print(f\"Baseline predictions - Min: {pred_baseline.min():.4f}, Max: {pred_baseline.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3909902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NC-CSF predictions - Mean: -1.0821, Std: 1.3467\n",
      "NC-CSF predictions - Min: -5.5421, Max: 3.3506\n"
     ]
    }
   ],
   "source": [
    "nccsf = NCCausalForestDML(n_estimators=200, min_samples_leaf=20, cv=5, random_state=42)\n",
    "nccsf.fit(Y=Y_train, T=A_train, X=X_train, Z=Z_train, W=W_train)\n",
    "pred_nccsf = nccsf.effect(X_test).ravel()\n",
    "\n",
    "print(f\"\\nNC-CSF predictions - Mean: {pred_nccsf.mean():.4f}, Std: {pred_nccsf.std():.4f}\")\n",
    "print(f\"NC-CSF predictions - Min: {pred_nccsf.min():.4f}, Max: {pred_nccsf.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616d477",
   "metadata": {},
   "source": [
    "### Generate semi-syn (use real X, A, Z, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d316e",
   "metadata": {},
   "source": [
    "**Severity Score Construction for Unmeasured Confounder $U$**\n",
    "\n",
    "We construct $U$ as a latent severity score from physiological lab values following a Gaussian factor model:\n",
    "\n",
    "1. **Standardize lab values**: For each lab measurement $L_j$ (where $j \\in \\{\\text{sod1, pot1, crea1, bili1, alb1, pafi1, paco21, ph1, hema1}\\}$):\n",
    "   $$\\tilde{L}_j = \\frac{L_j - \\bar{L}_j}{s_j}$$\n",
    "\n",
    "2. **Construct weighted score**: Using PCA to obtain weights $w_j$ from the first principal component:\n",
    "   $$U_i = \\sum_{j} w_j \\tilde{L}_{ij} + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma_U^2)$$\n",
    "\n",
    "3. **Standardize**: Final $U$ is standardized to have mean 0 and variance 1.\n",
    "\n",
    "This creates a realistic unmeasured confounder that represents underlying patient severity, influencing both treatment assignment and outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5d395586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 5735\n",
      "Severity score (U) - Mean: 0.0000, Std: 1.0000\n",
      "Target censoring rate: 0.00%\n",
      "Actual censoring rate: 0.00%\n",
      "Treatment proportion: 38.08%\n",
      "Mean time: 111.95 days\n",
      "Mean CATE: 46.1049\n",
      "tau_log_hr (imposed): -0.6000, Hazard Ratio: 0.5488\n",
      "\n",
      "Observed data shape: (5735, 14)\n",
      "Columns: ['A', 'age', 'sex', 'cat1_coma', 'cat2_coma', 'dnr1', 'surv2md1', 'aps1', 'pafi1', 'paco21', 'ph1', 'hema1', 'time', 'event']\n",
      "\n",
      "Truth data shape: (5735, 25)\n",
      "Columns: ['U', 'A', 'age', 'sex', 'cat1_coma', 'cat2_coma', 'dnr1', 'surv2md1', 'aps1', 'pafi1', 'paco21', 'ph1', 'hema1', 'time', 'event', 'T0', 'T1', 'C0', 'C1', 'T', 'C', 'eta_t0', 'eta_t1', 'CATE_XU', 'ITE']\n",
      "\n",
      "First few rows of observed data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cat1_coma</th>\n",
       "      <th>cat2_coma</th>\n",
       "      <th>dnr1</th>\n",
       "      <th>surv2md1</th>\n",
       "      <th>aps1</th>\n",
       "      <th>pafi1</th>\n",
       "      <th>paco21</th>\n",
       "      <th>ph1</th>\n",
       "      <th>hema1</th>\n",
       "      <th>time</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70.25098</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.640991</td>\n",
       "      <td>46</td>\n",
       "      <td>68.00000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>7.359375</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>95.064068</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>78.17896</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>50</td>\n",
       "      <td>218.31250</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7.329102</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>187.705238</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>46.09198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.317000</td>\n",
       "      <td>82</td>\n",
       "      <td>275.50000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.359375</td>\n",
       "      <td>21.097656</td>\n",
       "      <td>200.769915</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>75.33197</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.440979</td>\n",
       "      <td>48</td>\n",
       "      <td>156.65625</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7.459961</td>\n",
       "      <td>26.296875</td>\n",
       "      <td>48.647519</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>67.90997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.437000</td>\n",
       "      <td>72</td>\n",
       "      <td>478.00000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.229492</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>19.718504</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A       age  sex  cat1_coma  cat2_coma  dnr1  surv2md1  aps1      pafi1  \\\n",
       "0  0  70.25098    0          0          0     0  0.640991    46   68.00000   \n",
       "1  1  78.17896    1          0          0     0  0.755000    50  218.31250   \n",
       "2  1  46.09198    1          0          0     0  0.317000    82  275.50000   \n",
       "3  0  75.33197    1          0          0     0  0.440979    48  156.65625   \n",
       "4  1  67.90997    0          0          0     1  0.437000    72  478.00000   \n",
       "\n",
       "   paco21       ph1      hema1        time  event  \n",
       "0    40.0  7.359375  58.000000   95.064068      1  \n",
       "1    34.0  7.329102  32.500000  187.705238      1  \n",
       "2    16.0  7.359375  21.097656  200.769915      1  \n",
       "3    30.0  7.459961  26.296875   48.647519      1  \n",
       "4    17.0  7.229492  24.000000   19.718504      1  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from nc_csf.data_generation import weibull_ph_time_paper, sigmoid\n",
    "\n",
    "def generate_semi_synthetic_data(\n",
    "    analysis_df,\n",
    "    original_df,\n",
    "    severity_columns=None,\n",
    "    use_pca=True,\n",
    "    sigma_u=0.3,\n",
    "    linear_outcome=True,\n",
    "    target_censor_rate=0,\n",
    "    k_t=1.5,\n",
    "    lam_t=100.0,        # Baseline scale - controls typical survival time\n",
    "    tau_log_hr=-0.6,    # Log hazard ratio for treatment effect\n",
    "    beta_u_in_t=0.3,    # Effect of unmeasured confounder U on outcome (reduced from 0.8)\n",
    "    k_c=1.2,\n",
    "    lam_c=None,\n",
    "    beta_u_in_c=0.2,    # Effect of U on censoring (reduced from 0.3)\n",
    "    censor_lam_lo=1e-8,\n",
    "    censor_lam_hi=1e6,\n",
    "    max_censor_calib_iter=60,\n",
    "    admin_censor_time=None,\n",
    "    seed=123\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate semi-synthetic survival data using real X, A, Z, W and synthetic Y.\n",
    "    U is constructed as a severity score from physiological variables (unmeasured confounder).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    analysis_df : pd.DataFrame\n",
    "        DataFrame containing real X, A, Z, W columns (after dropna)\n",
    "    original_df : pd.DataFrame\n",
    "        Original DataFrame before dropna (for accessing severity columns)\n",
    "    severity_columns : list\n",
    "        List of column names to use for severity score U\n",
    "        Default: ['sod1', 'pot1', 'crea1', 'bili1', 'alb1', 'pafi1', 'paco21', 'ph1', 'hema1']\n",
    "    use_pca : bool\n",
    "        If True, use first principal component for weights; if False, use equal weights\n",
    "    sigma_u : float\n",
    "        Standard deviation of noise added to severity score (default: 0.3)\n",
    "   \n",
    "    Returns:\n",
    "    --------\n",
    "    observed_df : pd.DataFrame\n",
    "        DataFrame with semi-synthetic observed data (real X,A,Z,W and synthetic time, event)\n",
    "    truth_df : pd.DataFrame\n",
    "        DataFrame with additional ground truth columns (U, T0, T1, C0, C1, CATE, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    if severity_columns is None:\n",
    "        severity_columns = ['sod1', 'pot1', 'crea1', 'bili1', 'alb1', 'pafi1', 'paco21', 'ph1', 'hema1']\n",
    "    \n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(analysis_df)\n",
    "    \n",
    "    # Extract real data from analysis_df\n",
    "    A = analysis_df[\"A\"].values.astype(int)\n",
    "    X_colnames = [col for col in analysis_df.columns if col not in [\"Y\", \"A\", \"pafi1\", \"paco21\", \"ph1\", \"hema1\"]]\n",
    "    X_raw = analysis_df[X_colnames].values\n",
    "    Z = analysis_df[[\"pafi1\", \"paco21\"]].values\n",
    "    W = analysis_df[[\"ph1\", \"hema1\"]].values\n",
    "    \n",
    "    # CRITICAL: Standardize covariates to prevent extreme survival times\n",
    "    # Without standardization, large values (age≈70, aps1≈50) create very negative η\n",
    "    X_scaler = StandardScaler()\n",
    "    X = X_scaler.fit_transform(X_raw)\n",
    "    \n",
    "    p = X.shape[1]\n",
    "    \n",
    "    # Get indices of analysis_df in original_df to match rows\n",
    "    # We need to align the rows properly\n",
    "    analysis_indices = analysis_df.index\n",
    "    \n",
    "    # Extract severity columns from original_df using the same indices\n",
    "    severity_data = original_df.loc[analysis_indices, severity_columns].copy()\n",
    "    \n",
    "    # Handle missing values in severity columns (if any remain)\n",
    "    severity_data = severity_data.fillna(severity_data.mean())\n",
    "    \n",
    "    # Standardize the lab values: L_tilde = (L - mean(L)) / std(L)\n",
    "    scaler = StandardScaler()\n",
    "    L_tilde = scaler.fit_transform(severity_data)\n",
    "    \n",
    "    # Create severity score U (unmeasured confounder)\n",
    "    if use_pca:\n",
    "        # Use first principal component direction as weights\n",
    "        pca = PCA(n_components=1, random_state=seed)\n",
    "        U_score = pca.fit_transform(L_tilde).ravel()\n",
    "    else:\n",
    "        # Use equal weights\n",
    "        U_score = L_tilde.mean(axis=1)\n",
    "    \n",
    "    # Add Gaussian noise: U = weighted_sum + epsilon, epsilon ~ N(0, sigma_u^2)\n",
    "    epsilon = rng.normal(scale=sigma_u, size=n)\n",
    "    U = U_score + epsilon\n",
    "    \n",
    "    # Standardize U to have mean 0 and std 1 for consistency\n",
    "    U = (U - U.mean()) / U.std()\n",
    "    \n",
    "    # 4) Potential event times T0,T1 (shared uniform u_t)\n",
    "    # Use smaller scale for beta since X is now standardized\n",
    "    beta_t = rng.normal(scale=0.1, size=p)\n",
    "    u_t = rng.random(n)\n",
    "\n",
    "    beta_squared = None\n",
    "    beta_interact = None\n",
    "\n",
    "    if linear_outcome:\n",
    "        # Linear: standard Cox PH model\n",
    "        eta_t0 = X @ beta_t + beta_u_in_t * U + tau_log_hr * 0.0\n",
    "        eta_t1 = X @ beta_t + beta_u_in_t * U + tau_log_hr * 1.0\n",
    "    else:\n",
    "        # Non-linear: add non-linear transformations\n",
    "        # Use polynomial terms and sigmoid transformations\n",
    "        X_squared = X[:, :min(2, p)] ** 2  # squared terms for first few features\n",
    "        X_interact = X[:, 0:1] * U.reshape(-1, 1)  # interaction with U\n",
    "        \n",
    "        beta_squared = rng.normal(scale=0.2, size=X_squared.shape[1])\n",
    "        beta_interact = rng.normal(scale=0.2, size=X_interact.shape[1])\n",
    "        \n",
    "        nonlinear_part = (X_squared @ beta_squared + \n",
    "                         X_interact @ beta_interact + \n",
    "                         0.5 * sigmoid(U))\n",
    "        \n",
    "        eta_t0 = X @ beta_t + beta_u_in_t * U + nonlinear_part + tau_log_hr * 0.0\n",
    "        eta_t1 = X @ beta_t + beta_u_in_t * U + nonlinear_part + tau_log_hr * 1.0\n",
    "\n",
    "    T0 = weibull_ph_time_paper(u_t, k=k_t, lam=lam_t, eta=eta_t0)\n",
    "    T1 = weibull_ph_time_paper(u_t, k=k_t, lam=lam_t, eta=eta_t1)\n",
    "\n",
    "    # 5) Censoring times (use smaller beta scale for standardized X)\n",
    "    beta_c = rng.normal(scale=0.1, size=p)\n",
    "    u_c = rng.random(n)\n",
    "    eta_c = X @ beta_c + beta_u_in_c * U\n",
    "\n",
    "    T_obs_for_calib = np.where(A == 1, T1, T0)\n",
    "    lam_c_used = lam_c\n",
    "\n",
    "    if lam_c_used is None:\n",
    "        lo, hi = float(censor_lam_lo), float(censor_lam_hi)\n",
    "        for _ in range(max_censor_calib_iter):\n",
    "            mid = 0.5 * (lo + hi)\n",
    "            C_mid = weibull_ph_time_paper(u_c, k=k_c, lam=mid, eta=eta_c)\n",
    "            censor_rate_mid = (C_mid < T_obs_for_calib).mean()\n",
    "            if censor_rate_mid < target_censor_rate:\n",
    "                hi = mid\n",
    "            else:\n",
    "                lo = mid\n",
    "        lam_c_used = 0.5 * (lo + hi)\n",
    "\n",
    "    C0 = weibull_ph_time_paper(u_c, k=k_c, lam=lam_c_used, eta=eta_c)\n",
    "    C1 = weibull_ph_time_paper(u_c, k=k_c, lam=lam_c_used, eta=eta_c)\n",
    "\n",
    "    # 6) Realized T,C and observed (time,event)\n",
    "    T = np.where(A == 1, T1, T0)\n",
    "    C = np.where(A == 1, C1, C0)\n",
    "\n",
    "    time = np.minimum(T, C)\n",
    "    event = (T <= C).astype(int)\n",
    "\n",
    "    if admin_censor_time is not None:\n",
    "        admin = float(admin_censor_time)\n",
    "        cens_by_admin = admin < time\n",
    "        time = np.where(cens_by_admin, admin, time)\n",
    "        event = np.where(cens_by_admin, 0, event).astype(int)\n",
    "\n",
    "    # Create observed DataFrame (U is NOT included as it's unmeasured)\n",
    "    observed_df = analysis_df.copy()\n",
    "    observed_df[\"time\"] = time\n",
    "    observed_df[\"event\"] = event\n",
    "    \n",
    "    # Drop original Y if it exists, replace with new outcome\n",
    "    if \"Y\" in observed_df.columns:\n",
    "        observed_df = observed_df.drop(columns=[\"Y\"])\n",
    "    \n",
    "    # Create truth DataFrame with additional ground truth information (including U)\n",
    "    truth_df = observed_df.copy()\n",
    "    truth_df.insert(0, \"U\", U)\n",
    "    truth_df[\"T0\"] = T0\n",
    "    truth_df[\"T1\"] = T1\n",
    "    truth_df[\"C0\"] = C0\n",
    "    truth_df[\"C1\"] = C1\n",
    "    truth_df[\"T\"] = T\n",
    "    truth_df[\"C\"] = C\n",
    "    truth_df[\"eta_t0\"] = eta_t0\n",
    "    truth_df[\"eta_t1\"] = eta_t1\n",
    "    \n",
    "    # Add ground truth CATE\n",
    "    # For Weibull PH model: E[T | η] = λ * Γ(1 + 1/k) * exp(-η/k)\n",
    "    # CATE = E[T(1) - T(0) | X,U] = λ * Γ(1 + 1/k) * (exp(-η₁/k) - exp(-η₀/k))\n",
    "    G = math.gamma(1.0 + 1.0 / k_t)\n",
    "    cate_xu = lam_t * G * (np.exp(-eta_t1 / k_t) - np.exp(-eta_t0 / k_t))\n",
    "    truth_df[\"CATE_XU\"] = cate_xu\n",
    "    truth_df[\"ITE\"] = T1 - T0\n",
    "    \n",
    "    actual_censor_rate = (event == 0).mean()\n",
    "    print(f\"Sample size: {n}\")\n",
    "    print(f\"Severity score (U) - Mean: {U.mean():.4f}, Std: {U.std():.4f}\")\n",
    "    print(f\"Target censoring rate: {target_censor_rate:.2%}\")\n",
    "    print(f\"Actual censoring rate: {actual_censor_rate:.2%}\")\n",
    "    print(f\"Treatment proportion: {A.mean():.2%}\")\n",
    "    print(f\"Mean time: {time.mean():.2f} days\")\n",
    "    print(f\"Mean CATE: {cate_xu.mean():.4f}\")\n",
    "    print(f\"tau_log_hr (imposed): {tau_log_hr:.4f}, Hazard Ratio: {np.exp(tau_log_hr):.4f}\")\n",
    "    \n",
    "    return observed_df, truth_df\n",
    "\n",
    "# Generate semi-synthetic dataset with severity score U\n",
    "observed_df, truth_df = generate_semi_synthetic_data(\n",
    "    analysis_df=analysis_df,\n",
    "    original_df=df,\n",
    "    severity_columns=['sod1', 'pot1', 'crea1', 'bili1', 'alb1', 'pafi1', 'paco21', 'ph1', 'hema1'],\n",
    "    use_pca=True,\n",
    "    sigma_u=0.3,\n",
    "    target_censor_rate=0,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\nObserved data shape:\", observed_df.shape)\n",
    "print(\"Columns:\", list(observed_df.columns))\n",
    "print(\"\\nTruth data shape:\", truth_df.shape)\n",
    "print(\"Columns:\", list(truth_df.columns))\n",
    "print(\"\\nFirst few rows of observed data:\")\n",
    "observed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0e318fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4014\n",
      "Test samples: 1721\n",
      "Event rate (train): 100.00%\n",
      "Event rate (test): 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Split semi-synthetic data into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract variables from observed_df and truth_df\n",
    "X_cols = [col for col in observed_df.columns if col not in [\"time\", \"event\", \"A\", \"pafi1\", \"paco21\", \"ph1\", \"hema1\"]]\n",
    "X_semi = observed_df[X_cols].values\n",
    "A_semi = observed_df[\"A\"].values\n",
    "Y_semi = observed_df[\"time\"].values\n",
    "event_semi = observed_df[\"event\"].values\n",
    "Z_semi = observed_df[[\"pafi1\", \"paco21\"]].values\n",
    "W_semi = observed_df[[\"ph1\", \"hema1\"]].values\n",
    "U_semi = truth_df[\"U\"].values  # Ground truth unmeasured confounder\n",
    "CATE_true = truth_df[\"CATE_XU\"].values  # Ground truth CATE\n",
    "\n",
    "# Train/test split\n",
    "X_train_semi, X_test_semi, \\\n",
    "A_train_semi, A_test_semi, \\\n",
    "Y_train_semi, Y_test_semi, \\\n",
    "event_train_semi, event_test_semi, \\\n",
    "Z_train_semi, Z_test_semi, \\\n",
    "W_train_semi, W_test_semi, \\\n",
    "U_train_semi, U_test_semi, \\\n",
    "CATE_train_true, CATE_test_true = train_test_split(\n",
    "    X_semi, A_semi, Y_semi, event_semi, Z_semi, W_semi, U_semi, CATE_true,\n",
    "    test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train_semi)}\")\n",
    "print(f\"Test samples: {len(X_test_semi)}\")\n",
    "print(f\"Event rate (train): {event_train_semi.mean():.2%}\")\n",
    "print(f\"Event rate (test): {event_test_semi.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ca988421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASELINE CAUSAL FOREST\n",
      "============================================================\n",
      "\n",
      "Baseline predictions - Mean: 43.1096, Std: 7.2258\n",
      "Ground truth CATE   - Mean: 46.0786, Std: 13.8817\n",
      "Prediction error (RMSE): 15.0995\n"
     ]
    }
   ],
   "source": [
    "# 1. Baseline Causal Forest (ignores unmeasured confounding)\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE CAUSAL FOREST\")\n",
    "print(\"=\"*60)\n",
    "baseline_semi = BaselineCausalForestDML(n_estimators=200, min_samples_leaf=20, random_state=42)\n",
    "baseline_semi.fit_baseline(X_train_semi, A_train_semi, Y_train_semi, verbose=False)\n",
    "pred_baseline_semi = baseline_semi.effect(X_test_semi).ravel()\n",
    "\n",
    "print(f\"\\nBaseline predictions - Mean: {pred_baseline_semi.mean():.4f}, Std: {pred_baseline_semi.std():.4f}\")\n",
    "print(f\"Ground truth CATE   - Mean: {CATE_test_true.mean():.4f}, Std: {CATE_test_true.std():.4f}\")\n",
    "print(f\"Prediction error (RMSE): {np.sqrt(((pred_baseline_semi - CATE_test_true)**2).mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9497d916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "NC-CSF (Negative Control Causal Forest)\n",
      "============================================================\n",
      "\n",
      "NC-CSF predictions  - Mean: 43.3527, Std: 7.2830\n",
      "Ground truth CATE   - Mean: 46.0786, Std: 13.8817\n",
      "Prediction error (RMSE): 15.0935\n"
     ]
    }
   ],
   "source": [
    "# 2. NC-CSF (uses negative control proxies Z, W)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NC-CSF (Negative Control Causal Forest)\")\n",
    "print(\"=\"*60)\n",
    "nccsf_semi = NCCausalForestDML(n_estimators=200, min_samples_leaf=20, cv=5, random_state=42)\n",
    "nccsf_semi.fit(Y=Y_train_semi, T=A_train_semi, X=X_train_semi, Z=Z_train_semi, W=W_train_semi)\n",
    "pred_nccsf_semi = nccsf_semi.effect(X_test_semi).ravel()\n",
    "\n",
    "print(f\"\\nNC-CSF predictions  - Mean: {pred_nccsf_semi.mean():.4f}, Std: {pred_nccsf_semi.std():.4f}\")\n",
    "print(f\"Ground truth CATE   - Mean: {CATE_test_true.mean():.4f}, Std: {CATE_test_true.std():.4f}\")\n",
    "print(f\"Prediction error (RMSE): {np.sqrt(((pred_nccsf_semi - CATE_test_true)**2).mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1c08a1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ORACLE (uses true U)\n",
      "============================================================\n",
      "\n",
      "Oracle predictions  - Mean: 42.1043, Std: 6.8259\n",
      "Ground truth CATE   - Mean: 46.0786, Std: 13.8817\n",
      "Prediction error (RMSE): 14.9316\n"
     ]
    }
   ],
   "source": [
    "# 3. Oracle (uses true unmeasured confounder U - only possible with semi-synthetic data!)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ORACLE (uses true U)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "oracle_semi = NCCausalForestDMLOracle(n_estimators=200, min_samples_leaf=20, cv=5, random_state=42)\n",
    "oracle_semi.fit(Y=Y_train_semi, T=A_train_semi, X=X_train_semi, W=U_train_semi)\n",
    "pred_oracle_semi = oracle_semi.effect(X_test_semi).ravel()\n",
    "\n",
    "print(f\"\\nOracle predictions  - Mean: {pred_oracle_semi.mean():.4f}, Std: {pred_oracle_semi.std():.4f}\")\n",
    "print(f\"Ground truth CATE   - Mean: {CATE_test_true.mean():.4f}, Std: {CATE_test_true.std():.4f}\")\n",
    "print(f\"Prediction error (RMSE): {np.sqrt(((pred_oracle_semi - CATE_test_true)**2).mean()):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
