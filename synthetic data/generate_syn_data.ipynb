{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb4283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "import warnings\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Optional\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328016c1",
   "metadata": {},
   "source": [
    "## proxy strength (more or less informative about U): sigma_z, sigma_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6fc54f",
   "metadata": {},
   "source": [
    "| Corr(Z, U) | σ (sigma) |\n",
    "|------------|-----------|\n",
    "| 0.9        | 0.73      |\n",
    "| 0.8        | 1.125     |\n",
    "| 0.7        | 1.53      |\n",
    "| 0.6        | 2.0       |\n",
    "| 0.5        | 2.6       |\n",
    "| 0.4        | 3.44      |\n",
    "| 0.3        | 4.77      |\n",
    "| 0.2        | 7.35      |\n",
    "| 0.1        | 14.93     |\n",
    "\n",
    "* we fix $a_z, a_v = 1.5$ for all cases.\n",
    "* The table for sigma_v are the same as sigma_z."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa971a4a",
   "metadata": {},
   "source": [
    "## strength of confounding (how much U affects treatment assignment W and outcome T): gamma_u_in_w, beta_u_in_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c584cde9",
   "metadata": {},
   "source": [
    "\n",
    "| level    | gamma_u_in_w | beta_u_in_t |\n",
    "|----------|---------|--------|\n",
    "| none     | 0.0     | 0.0    |\n",
    "| weak     | 0.2     | 0.2    |\n",
    "| moderate | 0.5     | 0.5    |\n",
    "| strong   | 1.0     | 1.0    |\n",
    "| extreme  | 2.0     | 2.0    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbfc327",
   "metadata": {},
   "source": [
    "## strength of treatment effect: tau_log_hr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce951cf",
   "metadata": {},
   "source": [
    "| tau_log_hr value | Interpretation                          |\n",
    "|------------------|------------------------------------------|\n",
    "| 0                | No treatment effect                      |\n",
    "| < 0              | Beneficial (lower hazard)      |\n",
    "| > 0              | Harmful (higher hazard)        |\n",
    "\n",
    "**possible range**\n",
    "\n",
    "| Effect size category        | tau_log_hr range | Hazard ratio range (exp(tau)) | Interpretation |\n",
    "|----------------------------|------------------|-------------------------------|----------------|\n",
    "| Null / negligible          | [-0.1, 0.1]      | [0.90, 1.11]                  | Little to no effect |\n",
    "| Small to moderate (typical)| [-0.3, 0.3]      | [0.74, 1.35]                  | Plausible clinical effects |\n",
    "| Moderate to large          | [-0.7, 0.7]      | [0.50, 2.01]                  | Strong but still realistic |\n",
    "| Extreme (use with caution) | < -1 or > 1      | < 0.37 or > 2.7               | Often unrealistic / unstable |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5e743",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sigmoid function.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def calibrate_intercept_for_prevalence(\n",
    "    linpred_no_intercept: np.ndarray,\n",
    "    target_prevalence: float,\n",
    "    max_iter: int = 60,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Find an intercept b0 such that the average treatment probability \n",
    "    equals a desired prevalence.\n",
    "    \"\"\"\n",
    "    lo, hi = -20.0, 20.0\n",
    "    for _ in range(max_iter):\n",
    "        mid = 0.5 * (lo + hi)\n",
    "        p = sigmoid(mid + linpred_no_intercept).mean()\n",
    "        if p < target_prevalence:\n",
    "            lo = mid\n",
    "        else:\n",
    "            hi = mid\n",
    "    return 0.5 * (lo + hi)\n",
    "\n",
    "def weibull_ph_time_paper(\n",
    "    u01: np.ndarray, \n",
    "    k: float, \n",
    "    lam: float,\n",
    "    eta: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates survival times from a Weibull proportional hazards model, \n",
    "    using inverse-CDF sampling.\n",
    "    \"\"\"\n",
    "    u01 = np.clip(u01, 1e-12, 1 - 1e-12)\n",
    "    scale = lam * np.exp(-eta / k)\n",
    "    return scale * (-np.log(u01)) ** (1.0 / k)\n",
    "\n",
    "@dataclass\n",
    "class SynthConfig:\n",
    "    n: int = 5000                               # sample size\n",
    "    p_x: int = 10                               # number of covariates\n",
    "    seed: int = 123\n",
    "\n",
    "    # Treatment (W_i) assignment parameters\n",
    "    w_prevalence: float = 0.5                   # target treatment prevalence\n",
    "    gamma_u_in_w: float = 1.0                   # strength of unmeasured confounding in treatment (gamma_U)\n",
    "\n",
    "    # Survival (T_i(w)) outcome parameters\n",
    "    k_t: float = 1.5                            # Weibull shape\n",
    "    lam_t: float = 0.4                          # Weibull scale\n",
    "    tau_log_hr: float = -0.6                    # log hazard ratio for treatment effect (tau)\n",
    "    beta_u_in_t: float = 0.8                    # strength of unmeasured confounding in outcome (beta_U)\n",
    "    \n",
    "    # Censoring parameters\n",
    "    k_c: float = 1.2                            # Weibull shape\n",
    "    lam_c: Optional[float] = None               # Weibull scale (if None, will be calibrated)\n",
    "    beta_u_in_c: float = 0.3                    # strength of unmeasured confounding in censoring (beta_U)\n",
    "    target_censor_rate: float = 0.35\n",
    "    max_censor_calib_iter: int = 60             # iteration control for binary search when calibrating censoring\n",
    "    censor_lam_lo: float = 1e-8\n",
    "    censor_lam_hi: float = 1e6\n",
    "    admin_censor_time: Optional[float] = None   # a fixed administrative censoring cutoff time\n",
    "\n",
    "    # Negative control variables parameters\n",
    "    az: float = 1.5                             # coefficient for U in Z\n",
    "    av: float = 1.5                             # coefficient for U in V\n",
    "    sigma_z: float = 0.8                        # std dev of measurement error in Z\n",
    "    sigma_v: float = 0.8                        # std dev of measurement error in V\n",
    "\n",
    "@dataclass\n",
    "class SynthParams:\n",
    "\n",
    "    # Coefficient vectors of observed covariates X\n",
    "    b_z: np.ndarray\n",
    "    b_v: np.ndarray\n",
    "    beta_t: np.ndarray\n",
    "\n",
    "def generate_synthetic_nc_cox(cfg: SynthConfig) -> Tuple[pd.DataFrame, pd.DataFrame, SynthParams]:\n",
    "    \"\"\"\n",
    "    Generate synthetic survival data with:\n",
    "      - unmeasured confounding (latent U)\n",
    "      - treatment assignment confounded by U\n",
    "      - Weibull proportional hazards outcome model\n",
    "      - Weibull censoring model (possibly informative)\n",
    "      - two negative control variables Z and V\n",
    "\n",
    "    Returns:\n",
    "      observed_df: pd.DataFrame\n",
    "          What an analyst observes (time, event, W, X, Z, V)\n",
    "      truth_df: pd.DataFrame\n",
    "          Latent variables and counterfactual outcomes (for evaluation only)\n",
    "      params: SynthParams\n",
    "          True coefficient vectors used in the DGP\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(cfg.seed)\n",
    "    n, p = cfg.n, cfg.p_x\n",
    "\n",
    "    X = rng.normal(size=(n, p))\n",
    "    U = rng.normal(size=n)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Generate negative control variables Z and V\n",
    "    b_z = rng.normal(scale=0.3, size=p)\n",
    "    b_v = rng.normal(scale=0.3, size=p)\n",
    "\n",
    "    Z = cfg.az * U + X @ b_z + rng.normal(scale=cfg.sigma_z, size=n)\n",
    "    V = cfg.av * U + X @ b_v + rng.normal(scale=cfg.sigma_v, size=n)\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Generate treatment W\n",
    "    alpha = rng.normal(scale=0.5, size=p)\n",
    "    linpred = X @ alpha + cfg.gamma_u_in_w * U\n",
    "    b0 = calibrate_intercept_for_prevalence(linpred, cfg.w_prevalence)\n",
    "\n",
    "    p_w = sigmoid(b0 + linpred)\n",
    "    W = rng.binomial(1, p_w, size=n).astype(int)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Generate survival times T\n",
    "    beta_t = rng.normal(scale=0.4, size=p)\n",
    "    u_t = rng.random(n)\n",
    "\n",
    "    # ηi(w) = β_t^T * Xi +β_u * U_i +τw\n",
    "    eta_t0 = X @ beta_t + cfg.beta_u_in_t * U + cfg.tau_log_hr * 0.0\n",
    "    eta_t1 = X @ beta_t + cfg.beta_u_in_t * U + cfg.tau_log_hr * 1.0\n",
    "\n",
    "    T0 = weibull_ph_time_paper(u_t, k=cfg.k_t, lam=cfg.lam_t, eta=eta_t0)\n",
    "    T1 = weibull_ph_time_paper(u_t, k=cfg.k_t, lam=cfg.lam_t, eta=eta_t1)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Generate censoring times C\n",
    "    beta_c = rng.normal(scale=0.3, size=p)\n",
    "    u_c = rng.random(n)\n",
    "    \n",
    "    eta_c0 = X @ beta_c + cfg.beta_u_in_c * U\n",
    "    eta_c1 = X @ beta_c + cfg.beta_u_in_c * U\n",
    "\n",
    "    T_obs_for_calib = np.where(W == 1, T1, T0)\n",
    "    lam_c_used = cfg.lam_c\n",
    "    if lam_c_used is None:\n",
    "        lo, hi = float(cfg.censor_lam_lo), float(cfg.censor_lam_hi)\n",
    "        for _ in range(cfg.max_censor_calib_iter):\n",
    "            mid = 0.5 * (lo + hi)\n",
    "            C0_mid = weibull_ph_time_paper(u_c, k=cfg.k_c, lam=mid, eta=eta_c0)\n",
    "            C1_mid = weibull_ph_time_paper(u_c, k=cfg.k_c, lam=mid, eta=eta_c1)\n",
    "            C_obs_mid = np.where(W == 1, C1_mid, C0_mid)\n",
    "            censor_rate_mid = (C_obs_mid < T_obs_for_calib).mean()\n",
    "            if censor_rate_mid < cfg.target_censor_rate:\n",
    "                hi = mid\n",
    "            else:\n",
    "                lo = mid\n",
    "        lam_c_used = 0.5 * (lo + hi)\n",
    "\n",
    "    C0 = weibull_ph_time_paper(u_c, k=cfg.k_c, lam=lam_c_used, eta=eta_c0)\n",
    "    C1 = weibull_ph_time_paper(u_c, k=cfg.k_c, lam=lam_c_used, eta=eta_c1)\n",
    "\n",
    "    T = np.where(W == 1, T1, T0)\n",
    "    C = np.where(W == 1, C1, C0)\n",
    "    time = np.minimum(T, C)\n",
    "    event = (T <= C).astype(int)\n",
    "\n",
    "    if cfg.admin_censor_time is not None:\n",
    "        admin = float(cfg.admin_censor_time)\n",
    "        cens_by_admin = admin < time\n",
    "        time = np.where(cens_by_admin, admin, time)\n",
    "        event = np.where(cens_by_admin, 0, event).astype(int)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Assemble observed dataframe\n",
    "    X_cols = {f\"X{j}\": X[:, j] for j in range(p)}\n",
    "\n",
    "    observed_df = pd.DataFrame({\"time\": time, \"event\": event, \"W\": W, \"A\": W, \"Z\": Z, \"V\": V, **X_cols})\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Assemble truth dataframe\n",
    "    truth_df = observed_df.copy()\n",
    "    truth_df.insert(0, \"U\", U)\n",
    "    truth_df[\"T0\"] = T0\n",
    "    truth_df[\"T1\"] = T1\n",
    "    truth_df[\"C0\"] = C0\n",
    "    truth_df[\"C1\"] = C1\n",
    "    truth_df[\"T\"] = T\n",
    "    truth_df[\"C\"] = C\n",
    "    truth_df.attrs[\"lam_c_used\"] = lam_c_used\n",
    "\n",
    "    params = SynthParams(b_z=b_z, b_v=b_v, beta_t=beta_t)\n",
    "    return observed_df, truth_df, params\n",
    "\n",
    "def add_eq8_eq9_columns(\n",
    "    observed_df: pd.DataFrame,\n",
    "    truth_df: pd.DataFrame,\n",
    "    cfg: SynthConfig,\n",
    "    params: SynthParams,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    obs = observed_df.copy()\n",
    "    tru = truth_df.copy()\n",
    "    \"\"\"\n",
    "    Augment observed and truth dataframes with “oracle” / benchmarking quantities derived from the\n",
    "    negative-control Gaussian measurement model and the Weibull PH event-time model.\n",
    "\n",
    "    Returns:\n",
    "      obs: pd.DataFrame\n",
    "        Copy of observed_df with additional columns:\n",
    "        - tildeZ, tildeV\n",
    "        - mu_U_post, var_U_post\n",
    "        - CATE_XZV_eq9\n",
    "      tru: pd.DataFrame\n",
    "        Copy of truth_df with additional columns:\n",
    "        - CATE_XU_eq7\n",
    "        - ITE_T1_minus_T0\n",
    "    \"\"\"\n",
    "    x_cols = sorted([c for c in obs.columns if c.startswith(\"X\")], key=lambda s: int(s[1:]))\n",
    "    X = obs[x_cols].to_numpy()\n",
    "    Z = obs[\"Z\"].to_numpy()\n",
    "    V = obs[\"V\"].to_numpy()\n",
    "\n",
    "    # Z̃ = Z - X b_z\n",
    "    tildeZ = Z - X @ params.b_z\n",
    "    tildeV = V - X @ params.b_v\n",
    "\n",
    "    az, av = float(cfg.az), float(cfg.av)                           # coefficients for U in Z and V\n",
    "    sz2, sv2 = float(cfg.sigma_z) ** 2, float(cfg.sigma_v) ** 2     # variances of measurement errors\n",
    "    denom = (az**2) * sv2 + (av**2) * sz2 + sz2 * sv2               # common denominator\n",
    "\n",
    "    mu_post = (az * sv2 * tildeZ + av * sz2 * tildeV) / denom\n",
    "    var_post = (sz2 * sv2) / denom\n",
    "\n",
    "    k = float(cfg.k_t)\n",
    "    lam = float(cfg.lam_t)\n",
    "    tau = float(cfg.tau_log_hr)\n",
    "    beta_u = float(cfg.beta_u_in_t)\n",
    "\n",
    "    G = math.gamma(1.0 + 1.0 / k)                                   # Γ(1 + 1/k)                           \n",
    "    xb = X @ params.beta_t\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # oracle CATE formula conditioning on (X,Z,V) via the posterior of U\n",
    "    cate_xzv = (\n",
    "        lam * G * np.exp(-(1.0 / k) * xb -(beta_u / k) * mu_post + 0.5 * (beta_u**2) * var_post / (k**2))\n",
    "        * (np.exp(-tau / k) - 1.0)\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # oracle CATE formula conditioning on (X,U)\n",
    "    U = tru[\"U\"].to_numpy()\n",
    "    cate_xu = (lam * G * np.exp(-(1.0 / k) * (xb + beta_u * U)) * (np.exp(-tau / k) - 1.0))\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # true individual treatment effect on event time: T(1) - T(0)\n",
    "    ite = tru[\"T1\"].to_numpy() - tru[\"T0\"].to_numpy()\n",
    "\n",
    "    obs[\"tildeZ\"] = tildeZ\n",
    "    obs[\"tildeV\"] = tildeV\n",
    "    obs[\"mu_U_post\"] = mu_post\n",
    "    obs[\"var_U_post\"] = var_post\n",
    "    obs[\"CATE_XZV_eq9\"] = cate_xzv\n",
    "\n",
    "    tru[\"CATE_XU_eq7\"] = cate_xu\n",
    "    tru[\"ITE_T1_minus_T0\"] = ite\n",
    "\n",
    "    return obs, tru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c49d5d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10000\n",
      "Censoring rate: 0.000\n",
      "Treatment prevalence: 0.500\n",
      "\n",
      "Ground truth CATE (Eq.7, true U): mean=0.3266, std=0.4902\n",
      "Proxy-based CATE (Eq.9): mean=0.3261, std=0.4618\n"
     ]
    }
   ],
   "source": [
    "cfg = SynthConfig(\n",
    "    n=10000,\n",
    "    p_x=10,\n",
    "    lam_c=1e6,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "obs_df, truth_df, params = generate_synthetic_nc_cox(cfg)\n",
    "obs_df, truth_df = add_eq8_eq9_columns(obs_df, truth_df, cfg, params)\n",
    "\n",
    "print(f\"Dataset size: {len(obs_df)}\")\n",
    "print(f\"Censoring rate: {1 - obs_df['event'].mean():.3f}\")\n",
    "print(f\"Treatment prevalence: {obs_df['W'].mean():.3f}\")\n",
    "print(f\"\\nGround truth CATE (Eq.7, true U): mean={truth_df['CATE_XU_eq7'].mean():.4f}, std={truth_df['CATE_XU_eq7'].std():.4f}\")\n",
    "print(f\"Proxy-based CATE (Eq.9): mean={obs_df['CATE_XZV_eq9'].mean():.4f}, std={obs_df['CATE_XZV_eq9'].std():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
