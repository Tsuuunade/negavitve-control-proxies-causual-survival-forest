{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c7f013",
   "metadata": {},
   "source": [
    "# Choose fixed constants for synthetic causal survival data\n",
    "\n",
    "* In this case is no censoring, low proxy noise, weak confounding, strong treatment effect\n",
    "* If want to change:\n",
    "    * proxy noise: sigma_z, sigma_v\n",
    "    * strength of confounding: gamma_u, beta_u\n",
    "    * strength of treatment effect: tau\n",
    "    * level of censoring: target_censor_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a6e9d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants chosen:\n",
      "  n=5000, p=20\n",
      "  Confounding: gamma_u=0.2, beta_u=0.2\n",
      "  Proxy noise: a_z=a_v=1.5, sigma_z=sigma_v=0.6\n",
      "  Weibull shape k=1.5, baseline scale lambda0=1500.0\n",
      "  Admin censor time c_admin=1000.0, target censor rate=0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 123\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "\n",
    "\n",
    "# Dimensions\n",
    "n = 5000          # number of subjects\n",
    "p = 20            # number of covariates\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "# Proxy (negative control) model knobs: Z, V\n",
    "#   Z = a_z * U + b_z^T X + eps_z\n",
    "#   V = a_v * U + b_v^T X + eps_v\n",
    "#   Corr(Z, U) = a_z / sqrt(a_z^2 + sigma_z^2), same for Corr(V, U)\n",
    "#   -> sigma = a * sqrt(1 / Corr(Z,U)^2 - 1)\n",
    "\n",
    "# We fix a and tune sigma\n",
    "# Let a = 1.5\n",
    "#   then we have the following table:\n",
    "#   Corr(Z, U)          sigma\n",
    "#   0.9                 0.73\n",
    "#   0.8                 1.125\n",
    "#   0.7                 1.53\n",
    "#   0.6                 2.0\n",
    "#   0.5                 2.6\n",
    "#   0.4                 3.44\n",
    "#   0.3                 4.77\n",
    "#   0.2                 7.35\n",
    "#   0.1                 14.93\n",
    "\n",
    "# The lower the corr, the noisier the proxy\n",
    "a_z = 1.5\n",
    "a_v = 1.5\n",
    "sigma_z = 0.6\n",
    "sigma_v = 0.6\n",
    "\n",
    "# Make b_z, b_v sparse (a few nonzeros) with modest magnitude\n",
    "# Ensures signal(U) ≫ signal(X) ≫ noise\n",
    "\n",
    "# With p=20, choosing 5 nonzeros gives some heterogeneity (proxies aren’t trivial) but avoids dense, noisy relationships\n",
    "k_proxy = 5  # number of nonzero entries\n",
    "b_z = np.zeros(p)\n",
    "b_v = np.zeros(p)\n",
    "\n",
    "proxy_idx_z = rng.choice(p, size=k_proxy, replace=False)\n",
    "proxy_idx_v = rng.choice(p, size=k_proxy, replace=False)\n",
    "\n",
    "\n",
    "# Since X_i ~ N(0, I_p), each term contributes 0.2^2 = 0.04 to Var(b^T X)\n",
    "#   Total Var(b^T X) = 5 * 0.04 = 0.2\n",
    "#   Var(a_z U) = a^2 = 2.25\n",
    "#   U signal variance >> X signal variance\n",
    "b_z[proxy_idx_z] = 0.2\n",
    "b_v[proxy_idx_v] = 0.2\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Treatment assignment and survival knobs:\n",
    "#   P(W=1|X,U) = sigmoid(b0 + gamma_u * U + alpha^T X)\n",
    "#   eta(w) = beta_t^T X + beta_u * U + tau * w\n",
    "#   T(w) ~ Weibull(shape=k_weib, scale depends on eta(w))\n",
    "\n",
    "# Confounding strength controls how much U affects treatment assignment W and outcome T\n",
    "#   level     gamma_u     beta_u\n",
    "#   none      0.0         0.0\n",
    "#   weak      0.2         0.2\n",
    "#   moderate  0.5         0.5\n",
    "#   strong    1.0         1.0\n",
    "#   extreme   2.0         2.0       note: may need to change b0 to +- 0.3 to bring back the marginal treatment rate to ~0.5\n",
    "\n",
    "b0 = 0.0          # intercept (controls prevalence)\n",
    "gamma_u = 0.2     \n",
    "\n",
    "# alpha sparse with small magnitude\n",
    "# Ensures X matters, but not dominate; U still matters\n",
    "k_alpha = 5\n",
    "alpha = np.zeros(p)\n",
    "alpha_idx = rng.choice(p, size=k_alpha, replace=False)\n",
    "alpha[alpha_idx] = 0.1\n",
    "\n",
    "#   k           effect\n",
    "#   1           constant hazard (exponential)\n",
    "#   1.2-1.8     mildly increasing hazard (recommended range)\n",
    "#   >2.5        very fast hazard growth; short survival\n",
    "#   <1          decreasing hazard (not recommended)\n",
    "\n",
    "k_weib = 1.5      # Weibull shape\n",
    "beta_u = 0.2      \n",
    "\n",
    "# beta_t sparse with small magnitude\n",
    "# Ensures X matters, but not dominate; U still matters\n",
    "k_beta = 5\n",
    "beta_t = np.zeros(p)\n",
    "beta_idx = rng.choice(p, size=k_beta, replace=False)\n",
    "beta_t[beta_idx] = 0.1\n",
    "\n",
    "# Treatment effect on log-hazard\n",
    "# Changing tau changes how strong the treatment effect is\n",
    "#   hazard ratio = exp(tau)\n",
    "\n",
    "#   tau           effect\n",
    "#   0.0           no effect\n",
    "#   0.05-0.15     small effect\n",
    "#   0.15-0.3      moderate effect\n",
    "#   0.3-0.6       large effect\n",
    "\n",
    "tau = 0.2\n",
    "\n",
    "# Baseline scale parameter for event times\n",
    "lambda0 = 1500.0\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Censoring knobs\n",
    "target_censor_rate = 0\n",
    "c_admin = 1000.0  # administrative censoring time (days), initial guess\n",
    "\n",
    "\n",
    "# Add light random censoring (set to False for pure administrative censoring)\n",
    "# Pure adminstrative censoring: everyone is censored only because the study ends, not because of individual-specific dropout or loss to follow-up\n",
    "use_random_censoring = False\n",
    "rand_censor_dist = \"exponential\"  # placeholder, only used if use_random_censoring=True\n",
    "rand_censor_scale = 5000.0        # placeholder\n",
    "\n",
    "\n",
    "\n",
    "# Bundle parameters into a dictionary\n",
    "params = dict(\n",
    "    SEED=SEED, n=n, p=p,\n",
    "    a_z=a_z, a_v=a_v, sigma_z=sigma_z, sigma_v=sigma_v, b_z=b_z, b_v=b_v,\n",
    "    b0=b0, gamma_u=gamma_u, alpha=alpha,\n",
    "    k_weib=k_weib, lambda0=lambda0, beta_t=beta_t, beta_u=beta_u, tau=tau,\n",
    "    target_censor_rate=target_censor_rate, c_admin=c_admin,\n",
    "    use_random_censoring=use_random_censoring,\n",
    "    rand_censor_dist=rand_censor_dist, rand_censor_scale=rand_censor_scale\n",
    ")\n",
    "\n",
    "print(\"Constants chosen:\")\n",
    "print(f\"  n={n}, p={p}\")\n",
    "print(f\"  Confounding: gamma_u={gamma_u}, beta_u={beta_u}\")\n",
    "print(f\"  Proxy noise: a_z=a_v={a_z}, sigma_z=sigma_v={sigma_z}\")\n",
    "print(f\"  Weibull shape k={k_weib}, baseline scale lambda0={lambda0}\")\n",
    "print(f\"  Admin censor time c_admin={c_admin}, target censor rate={target_censor_rate}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025ec89a",
   "metadata": {},
   "source": [
    "# Draw observed covariates X_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3648a77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (5000, 20)\n",
      "X mean (first 5 dims): [-0.00417848 -0.00737974  0.03927416 -0.00916619  0.00587886]\n",
      "X std  (first 5 dims): [0.99882706 0.99480017 1.00976172 0.99426534 1.00630381]\n"
     ]
    }
   ],
   "source": [
    "# X_i ~ N(0, I_p)\n",
    "# Shape: (n, p)\n",
    "\n",
    "X = rng.normal(loc=0.0, scale=1.0, size=(n, p))\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"X mean (first 5 dims):\", X.mean(axis=0)[:5])\n",
    "print(\"X std  (first 5 dims):\", X.std(axis=0)[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358dd916",
   "metadata": {},
   "source": [
    "# Draw the unobserved confounder U_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42834a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U shape: (5000,)\n",
      "U mean: 0.0031244516699708\n",
      "U std : 0.9833456171370012\n"
     ]
    }
   ],
   "source": [
    "# U_i ~ N(0, 1)\n",
    "# Shape: (n,)\n",
    "\n",
    "U = rng.normal(loc=0.0, scale=1.0, size=n)\n",
    "\n",
    "print(\"U shape:\", U.shape)\n",
    "print(\"U mean:\", U.mean())\n",
    "print(\"U std :\", U.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b917ea",
   "metadata": {},
   "source": [
    "# Generate proxy variables Z_i and V_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8dca8128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z shape: (5000,) V shape: (5000,)\n",
      "Z summary (mean, std): -0.006995013255793667 1.6466043972342541\n",
      "V summary (mean, std): 0.004072923637241014 1.657567563693127\n",
      "Corr(Z, U): 0.8937167621086809\n",
      "Corr(V, U): 0.8886880525407779\n"
     ]
    }
   ],
   "source": [
    "# Z_i = a_z * U_i + b_z^T X_i + eps_z\n",
    "# V_i = a_v * U_i + b_v^T X_i + eps_v\n",
    "# eps_z ~ N(0, sigma_z^2), eps_v ~ N(0, sigma_v^2)\n",
    "\n",
    "eps_z = rng.normal(loc=0.0, scale=sigma_z, size=n)\n",
    "eps_v = rng.normal(loc=0.0, scale=sigma_v, size=n)\n",
    "\n",
    "Z = a_z * U + (X @ b_z) + eps_z\n",
    "V = a_v * U + (X @ b_v) + eps_v\n",
    "\n",
    "print(\"Z shape:\", Z.shape, \"V shape:\", V.shape)\n",
    "print(\"Z summary (mean, std):\", float(Z.mean()), float(Z.std()))\n",
    "print(\"V summary (mean, std):\", float(V.mean()), float(V.std()))\n",
    "print(\"Corr(Z, U):\", float(np.corrcoef(Z, U)[0, 1]))\n",
    "print(\"Corr(V, U):\", float(np.corrcoef(V, U)[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed36d19",
   "metadata": {},
   "source": [
    "# Assign treatment W_i using a confounded logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d45802c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W shape: (5000,)\n",
      "Treatment prevalence (mean W): 0.5112\n",
      "Propensity score range: (np.float64(0.2527443961888165), np.float64(0.7518095236861074))\n"
     ]
    }
   ],
   "source": [
    "# W_i ~ Bernoulli(sigmoid(b0 + alpha^T X_i + gamma_u * U_i))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# Linear predictor for treatment assignment\n",
    "lin_treat = b0 + X @ alpha + gamma_u * U\n",
    "\n",
    "# Propensity scores\n",
    "p_treat = sigmoid(lin_treat)\n",
    "\n",
    "# Draw treatment\n",
    "W = rng.binomial(n=1, p=p_treat, size=n).astype(int)\n",
    "\n",
    "print(\"W shape:\", W.shape)\n",
    "print(\"Treatment prevalence (mean W):\", W.mean())\n",
    "print(\"Propensity score range:\", (p_treat.min(), p_treat.max()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc00955c",
   "metadata": {},
   "source": [
    "# Generate potential event times T_i(0) and T_i(1) from a Weibull Cox proportional hazards model\n",
    "\n",
    "* If $U \\sim \\text{Uniform}(0,1)$, then $T = F^{-1}(U)$ has cumulative distribution function (CDF) $F$. Therefore, if you know the Weibull CDF for $T(w)$, you can generate $T(w)$ by plugging the same uniform draw $U$ into the corresponding inverse CDF.\n",
    "* Using the same $U_i$ for both $T_i(0)$ and $T_i(1)$ links the two potential outcomes for the same individual: If $U_i$ is large, both $T_i(0)$ and $T_i(1)$ will be large (late event) relative to their own distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a986c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T0/T1 shapes: (5000,) (5000,)\n",
      "Median T0, T1: 1157.71041711901 1013.197268240639\n",
      "Corr(T0, T1): 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Define the Cox-style linear predictors for each potential world w in {0,1}\n",
    "#    eta_i(w) = beta_t^T X_i + beta_u * U_i + tau * w\n",
    "eta0 = X @ beta_t + beta_u * U + tau * 0.0\n",
    "eta1 = X @ beta_t + beta_u * U + tau * 1.0\n",
    "\n",
    "# In the PDF parameterization:\n",
    "#    T_i(w) | X_i, U_i ~ Weibull(shape=k_weib, scale = lambda0 * exp(-eta_i(w) / k_weib))\n",
    "scale0 = lambda0 * np.exp(-eta0 / k_weib)\n",
    "scale1 = lambda0 * np.exp(-eta1 / k_weib)\n",
    "\n",
    "# Linking step:\n",
    "#    Draw one latent uniform per subject i.\n",
    "#    This is the \"rank\" of that person in their conditional event-time distribution.\n",
    "u_latent = rng.uniform(0.0, 1.0, size=n)\n",
    "u_latent = np.clip(u_latent, 1e-12, 1 - 1e-12)  # avoid numerical issues at 0 or 1\n",
    "\n",
    "# Weibull rv has CDF: F_w(t) = 1 - exp( -(t / scale_w)^k_weib )\n",
    "# If U ~ Uniform(0,1), then F^-1(U) has CDF F_w(t). So we solve u=F(t) for t\n",
    "#    T = scale_w * (-log(1-U))^(1/k_weib)\n",
    "base = (-np.log(1.0 - u_latent)) ** (1.0 / k_weib)\n",
    "T0 = scale0 * base\n",
    "T1 = scale1 * base\n",
    "\n",
    "print(\"T0/T1 shapes:\", T0.shape, T1.shape)\n",
    "print(\"Median T0, T1:\", np.median(T0), np.median(T1))\n",
    "\n",
    "# Should be 1 due to shared u_latent\n",
    "#   T1 = (scale1 / scale0) * T0\n",
    "#   This ratio depends only on tau, X, U, but not on any additional randomness -> linear relationship\n",
    "print(\"Corr(T0, T1):\", np.corrcoef(T0, T1)[0, 1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f72aea8",
   "metadata": {},
   "source": [
    "# Reveal the factual event time T_i under the assigned treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0176f9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T shape: (5000,)\n",
      "Observed event time summary (min, median, max): (np.float64(4.174980569630763), np.float64(1086.3393771408537), np.float64(8050.372317975076))\n",
      "Median T | W=0: 1174.7753817081002\n",
      "Median T | W=1: 994.7938404923486\n"
     ]
    }
   ],
   "source": [
    "# T_i = T_i(W_i)\n",
    "# Use treatment indicator W to select the corresponding potential outcome\n",
    "T = np.where(W == 1, T1, T0)\n",
    "\n",
    "print(\"T shape:\", T.shape)\n",
    "print(\"Observed event time summary (min, median, max):\",\n",
    "      (np.min(T), np.median(T), np.max(T)))\n",
    "print(\"Median T | W=0:\", np.median(T[W == 0]))\n",
    "print(\"Median T | W=1:\", np.median(T[W == 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce8b83",
   "metadata": {},
   "source": [
    "# Add right-censoring to get observed (Y_i, Δ_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ac0e382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated administrative censoring time c_admin = 8050.372\n",
      "Achieved censoring rate = 0.000%  (target 0%)\n",
      "Y summary (min, median, max): (4.174980569630763, 1086.3393771408537, 8050.372317975076)\n",
      "Events observed (Delta=1) count: 5000 out of 5000\n"
     ]
    }
   ],
   "source": [
    "# If we set c_admin to the (1 - target_censor_rate) quantile of T,\n",
    "# then approximately target_censor_rate fraction will satisfy T > c_admin and be censored.\n",
    "c_admin = float(np.quantile(T, 1.0 - target_censor_rate))\n",
    "\n",
    "# Censoring time (pure administrative censoring)\n",
    "C = np.full(n, c_admin, dtype=float)\n",
    "\n",
    "# (Optional) add light random censoring on top (usually leave off for clean control)\n",
    "# If you want it, set use_random_censoring=True in Cell 0\n",
    "if use_random_censoring:\n",
    "    if rand_censor_dist == \"exponential\":\n",
    "        C_rand = rng.exponential(scale=rand_censor_scale, size=n)\n",
    "    elif rand_censor_dist == \"uniform\":\n",
    "        C_rand = rng.uniform(low=0.0, high=rand_censor_scale, size=n)\n",
    "    else:\n",
    "        raise ValueError(\"rand_censor_dist must be 'exponential' or 'uniform'\")\n",
    "    C = np.minimum(C, C_rand)\n",
    "\n",
    "# Observed follow-up time and event indicator\n",
    "Y = np.minimum(T, C)\n",
    "Delta = (T <= C).astype(int)  # 1=event observed, 0=censored\n",
    "\n",
    "# Report achieved censoring\n",
    "censor_rate = 1.0 - Delta.mean()\n",
    "\n",
    "print(f\"Calibrated administrative censoring time c_admin = {c_admin:.3f}\")\n",
    "print(f\"Achieved censoring rate = {censor_rate:.3%}  (target {target_censor_rate:.0%})\")\n",
    "print(\"Y summary (min, median, max):\", (float(Y.min()), float(np.median(Y)), float(Y.max())))\n",
    "print(\"Events observed (Delta=1) count:\", int(Delta.sum()), \"out of\", n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1e659f",
   "metadata": {},
   "source": [
    "# Combine all generated components into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ac71afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed data shape: (5000, 25)\n",
      "Columns: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'Z', 'V', 'W', 'Y', 'Delta']\n",
      "\n",
      "Head of observed dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>Z</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>Y</th>\n",
       "      <th>Delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.311795</td>\n",
       "      <td>0.337769</td>\n",
       "      <td>-2.207471</td>\n",
       "      <td>0.827921</td>\n",
       "      <td>1.541630</td>\n",
       "      <td>1.126807</td>\n",
       "      <td>0.754770</td>\n",
       "      <td>-0.145978</td>\n",
       "      <td>1.281902</td>\n",
       "      <td>1.074031</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.172044</td>\n",
       "      <td>-0.370147</td>\n",
       "      <td>0.164380</td>\n",
       "      <td>0.859881</td>\n",
       "      <td>1.761661</td>\n",
       "      <td>1.321089</td>\n",
       "      <td>1.611579</td>\n",
       "      <td>0</td>\n",
       "      <td>1440.906398</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.993324</td>\n",
       "      <td>-0.291521</td>\n",
       "      <td>0.728128</td>\n",
       "      <td>-1.261600</td>\n",
       "      <td>1.429939</td>\n",
       "      <td>-0.156475</td>\n",
       "      <td>-0.673759</td>\n",
       "      <td>-0.639060</td>\n",
       "      <td>-0.061361</td>\n",
       "      <td>-0.392785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055346</td>\n",
       "      <td>-0.481563</td>\n",
       "      <td>-0.583408</td>\n",
       "      <td>-0.862161</td>\n",
       "      <td>-1.488175</td>\n",
       "      <td>-0.700023</td>\n",
       "      <td>-0.837553</td>\n",
       "      <td>0</td>\n",
       "      <td>605.352720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.216307</td>\n",
       "      <td>0.984376</td>\n",
       "      <td>-0.543084</td>\n",
       "      <td>-0.558615</td>\n",
       "      <td>-0.316483</td>\n",
       "      <td>-0.460640</td>\n",
       "      <td>-1.436270</td>\n",
       "      <td>1.365108</td>\n",
       "      <td>0.439000</td>\n",
       "      <td>-0.711695</td>\n",
       "      <td>...</td>\n",
       "      <td>1.519524</td>\n",
       "      <td>1.703909</td>\n",
       "      <td>-0.248859</td>\n",
       "      <td>-0.499749</td>\n",
       "      <td>0.099598</td>\n",
       "      <td>-0.883904</td>\n",
       "      <td>-0.878286</td>\n",
       "      <td>1</td>\n",
       "      <td>511.533413</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.128343</td>\n",
       "      <td>-0.734222</td>\n",
       "      <td>-0.620475</td>\n",
       "      <td>0.813274</td>\n",
       "      <td>1.641801</td>\n",
       "      <td>-0.226501</td>\n",
       "      <td>-0.647965</td>\n",
       "      <td>-0.283371</td>\n",
       "      <td>-0.995131</td>\n",
       "      <td>-0.272872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.148758</td>\n",
       "      <td>1.315666</td>\n",
       "      <td>-1.222346</td>\n",
       "      <td>-0.303591</td>\n",
       "      <td>-1.173689</td>\n",
       "      <td>2.035410</td>\n",
       "      <td>2.305684</td>\n",
       "      <td>0</td>\n",
       "      <td>1072.121843</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.826274</td>\n",
       "      <td>0.850322</td>\n",
       "      <td>-0.515768</td>\n",
       "      <td>1.658113</td>\n",
       "      <td>-0.297263</td>\n",
       "      <td>-1.383377</td>\n",
       "      <td>-0.281205</td>\n",
       "      <td>0.360021</td>\n",
       "      <td>-0.234392</td>\n",
       "      <td>2.265521</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.728486</td>\n",
       "      <td>-0.646397</td>\n",
       "      <td>1.115104</td>\n",
       "      <td>-0.843211</td>\n",
       "      <td>-0.636688</td>\n",
       "      <td>-0.237534</td>\n",
       "      <td>0.432936</td>\n",
       "      <td>0</td>\n",
       "      <td>922.758148</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0 -0.311795  0.337769 -2.207471  0.827921  1.541630  1.126807  0.754770   \n",
       "1  0.993324 -0.291521  0.728128 -1.261600  1.429939 -0.156475 -0.673759   \n",
       "2  0.216307  0.984376 -0.543084 -0.558615 -0.316483 -0.460640 -1.436270   \n",
       "3  0.128343 -0.734222 -0.620475  0.813274  1.641801 -0.226501 -0.647965   \n",
       "4  0.826274  0.850322 -0.515768  1.658113 -0.297263 -1.383377 -0.281205   \n",
       "\n",
       "         X8        X9       X10  ...       X16       X17       X18       X19  \\\n",
       "0 -0.145978  1.281902  1.074031  ... -2.172044 -0.370147  0.164380  0.859881   \n",
       "1 -0.639060 -0.061361 -0.392785  ...  0.055346 -0.481563 -0.583408 -0.862161   \n",
       "2  1.365108  0.439000 -0.711695  ...  1.519524  1.703909 -0.248859 -0.499749   \n",
       "3 -0.283371 -0.995131 -0.272872  ... -0.148758  1.315666 -1.222346 -0.303591   \n",
       "4  0.360021 -0.234392  2.265521  ... -2.728486 -0.646397  1.115104 -0.843211   \n",
       "\n",
       "        X20         Z         V  W            Y  Delta  \n",
       "0  1.761661  1.321089  1.611579  0  1440.906398      1  \n",
       "1 -1.488175 -0.700023 -0.837553  0   605.352720      1  \n",
       "2  0.099598 -0.883904 -0.878286  1   511.533413      1  \n",
       "3 -1.173689  2.035410  2.305684  0  1072.121843      1  \n",
       "4 -0.636688 -0.237534  0.432936  0   922.758148      1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame for covariates X\n",
    "X_cols = [f\"X{j+1}\" for j in range(p)]\n",
    "df_X = pd.DataFrame(X, columns=X_cols)\n",
    "\n",
    "# Full observed dataset\n",
    "df = pd.concat(\n",
    "    [\n",
    "        df_X,\n",
    "        pd.DataFrame({\n",
    "            \"Z\": Z,                 # proxy 1\n",
    "            \"V\": V,                 # proxy 2\n",
    "            \"W\": W,                 # treatment\n",
    "            \"Y\": Y,                 # observed time\n",
    "            \"Delta\": Delta          # event indicator (1=event, 0=censored)\n",
    "        })\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Hidden variables\n",
    "df_hidden = pd.DataFrame({\n",
    "    \"U\": U,\n",
    "    \"T0\": T0,\n",
    "    \"T1\": T1,\n",
    "    \"T\": T\n",
    "})\n",
    "\n",
    "print(\"Observed data shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"\\nHead of observed dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# A single object with everything (observed + hidden):\n",
    "full_data = {\n",
    "    \"observed\": df,\n",
    "    \"hidden\": df_hidden\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112756e1",
   "metadata": {},
   "source": [
    "# Compute CATE given (X, U): E[T(1)−T(0)∣X,U]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "869ccc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[T(0) | X, U] summary (min, median, max): (608.7524498092433, 1358.366383821356, 2694.2747038118105)\n",
      "E[T(1) | X, U] summary (min, median, max): (532.7639019750808, 1188.8060166053024, 2357.9573349484363)\n",
      "Max |direct - closed-form|: 5.115907697472721e-13\n",
      "CATE(X,U) summary (min, median, max): (-336.3173688633742, -169.56036721605358, -75.98854783416255)\n",
      "Average oracle CATE (given X,U): -172.36864269422128\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import gamma\n",
    "\n",
    "# Linear predictors (already conceptually defined earlier)\n",
    "eta0 = X @ beta_t + beta_u * U + tau * 0.0\n",
    "eta1 = X @ beta_t + beta_u * U + tau * 1.0\n",
    "\n",
    "# Constant factor from the Weibull mean\n",
    "weibull_mean_const = lambda0 * gamma(1.0 + 1.0 / k_weib)\n",
    "\n",
    "# Conditional mean event times\n",
    "E_T0_XU = weibull_mean_const * np.exp(-eta0 / k_weib)\n",
    "E_T1_XU = weibull_mean_const * np.exp(-eta1 / k_weib)\n",
    "\n",
    "print(\"E[T(0) | X, U] summary (min, median, max):\", (float(E_T0_XU.min()), float(np.median(E_T0_XU)), float(E_T0_XU.max())))\n",
    "print(\"E[T(1) | X, U] summary (min, median, max):\", (float(E_T1_XU.min()), float(np.median(E_T1_XU)), float(E_T1_XU.max())))\n",
    "\n",
    "# Option 1: directly from previous results\n",
    "CATE_XU_direct = E_T1_XU - E_T0_XU\n",
    "\n",
    "# Option 2: closed-form expression\n",
    "common_term = weibull_mean_const * np.exp(-(X @ beta_t + beta_u * U) / k_weib)\n",
    "CATE_XU_closed = common_term * (np.exp(-tau / k_weib) - 1.0)\n",
    "\n",
    "# Sanity check: abs diff should be small\n",
    "max_abs_diff = float(np.max(np.abs(CATE_XU_direct - CATE_XU_closed)))\n",
    "\n",
    "print(\"Max |direct - closed-form|:\", max_abs_diff)\n",
    "print(\"CATE(X,U) summary (min, median, max):\", (float(CATE_XU_direct.min()), float(np.median(CATE_XU_direct)), float(CATE_XU_direct.max())))\n",
    "print(\"Average oracle CATE (given X,U):\", float(np.mean(CATE_XU_direct)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09658a5",
   "metadata": {},
   "source": [
    "# Residualize proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f99a2dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corr(Z_tilde, U): 0.9249655839101226\n",
      "Corr(V_tilde, U): 0.9268329887829447\n"
     ]
    }
   ],
   "source": [
    "Z_tilde = Z - (X @ b_z)\n",
    "V_tilde = V - (X @ b_v)\n",
    "\n",
    "# These should remain strongly correlated with U if proxies are low-noise\n",
    "print(\"Corr(Z_tilde, U):\", float(np.corrcoef(Z_tilde, U)[0, 1]))\n",
    "print(\"Corr(V_tilde, U):\", float(np.corrcoef(V_tilde, U)[0, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dea08a",
   "metadata": {},
   "source": [
    "# Compute the conditional distribution of U given observed (X, Z, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ce75edde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_U_given_ZV shape: (5000,)\n",
      "var_U_given_ZV: 0.07407407407407408\n",
      "Corr(mu_U_given_ZV, U): 0.9606790664758929\n"
     ]
    }
   ],
   "source": [
    "numerator = a_z * (sigma_v ** 2) * Z_tilde + a_v * (sigma_z ** 2) * V_tilde\n",
    "\n",
    "denominator = (a_z ** 2) * (sigma_v ** 2) + (a_v ** 2) * (sigma_z ** 2) + (sigma_z ** 2) * (sigma_v ** 2)\n",
    "\n",
    "mu_U_given_ZV = numerator / denominator\n",
    "\n",
    "var_U_given_ZV = (sigma_z ** 2) * (sigma_v ** 2) / denominator\n",
    "\n",
    "print(\"mu_U_given_ZV shape:\", mu_U_given_ZV.shape)\n",
    "print(\"var_U_given_ZV:\", float(var_U_given_ZV))\n",
    "# correlation between posterior mean and true U (should be high if proxies are informative)\n",
    "print(\"Corr(mu_U_given_ZV, U):\", float(np.corrcoef(mu_U_given_ZV, U)[0, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f3505",
   "metadata": {},
   "source": [
    "# Get a fully observed “ground-truth” effect $E[T(1)−T(0)∣X,Z,V]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "21713d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATE_XZV shape: (5000,)\n",
      "CATE(X,Z,V) summary (min, median, max): (-341.76154348852765, -169.29553238718745, -72.29427870974213)\n",
      "Average benchmark CATE (given X,Z,V): -172.52084017001474\n"
     ]
    }
   ],
   "source": [
    "const_term = lambda0 * gamma(1.0 + 1.0 / k_weib)                # lambda * Gamma(1 + 1/k)\n",
    "x_term = -(1.0 / k_weib) * (X @ beta_t)                         # -(1/k) * beta_t^T X_i\n",
    "mu_term = -(beta_u / k_weib) * mu_U_given_ZV                    # -(beta_u/k) * mu_{U|Z,V}\n",
    "var_term = 0.5 * (beta_u**2 / (k_weib**2)) * var_U_given_ZV     # + 1/2 * (beta_u^2/k^2) * sigma^2_{U|Z,V}\n",
    "contrast_term = (np.exp(-tau / k_weib) - 1.0)                   # (e^{-tau/k} - 1)\n",
    "\n",
    "CATE_XZV = const_term * np.exp(x_term + mu_term + var_term) * contrast_term\n",
    "\n",
    "print(\"CATE_XZV shape:\", CATE_XZV.shape)\n",
    "print(\"CATE(X,Z,V) summary (min, median, max):\", (float(CATE_XZV.min()), float(np.median(CATE_XZV)), float(CATE_XZV.max())))\n",
    "print(\"Average benchmark CATE (given X,Z,V):\", float(np.mean(CATE_XZV)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea988ba",
   "metadata": {},
   "source": [
    "* $E[T(1)−T(0)∣X,U] = -172.37$\n",
    "* $E[T(1)−T(0)∣X,Z,V] = -172.52$\n",
    "* true causal effect at the individual level v.s. actual causal effect we can observe in real life"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb7a43c",
   "metadata": {},
   "source": [
    "不要censor\n",
    "\n",
    "比较我们的method on uncensored和eq 8， 比mean sq error\n",
    "\n",
    "* basic version of csf, using x v z as all covariates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
